#!pythonfrom libc.stdio cimport printf# KD Tree and Ball Tree# =====================##    Author: Jake Vanderplas <jakevdp@cs.washington.edu>, 2012-2013#    License: BSD## This file is meant to be a literal include in a pyx file.# See ball_tree.pyx and kd_tree.pyx## The routines here are the core algorithms of the KDTree and BallTree# structures.  If Cython supported polymorphism, we would be able to# create a subclass and derive KDTree and BallTree from it.  Because# polymorphism is not an option, we use this single BinaryTree class# as a literal include to avoid duplicating the entire file.## A series of functions are implemented in kd_tree.pyx and ball_tree.pyx# which use the information here to calculate the lower and upper bounds# between a node and a point, and between two nodes.  These functions are# used here, and are all that are needed to differentiate between the two# tree types.## Description of Binary Tree Algorithms# -------------------------------------# A binary tree can be thought of as a collection of nodes.  The top node# contains all the points.  The next level consists of two nodes with half# the points in each, and this continues recursively.  Each node contains# metadata which allow fast computation of distance bounds: in the case of# a ball tree, the metadata is a center and a radius.  In the case of a# KD tree, the metadata is the minimum and maximum bound along each dimension.## In a typical KD Tree or Ball Tree implementation, the nodes are implemented# as dynamically allocated structures with pointers linking them.  Here we# take a different approach, storing all relevant data in a set of arrays# so that the entire tree object can be saved in a pickle file. For efficiency,# the data can be stored in such a way that explicit pointers are not# necessary: for node data stored at index i, the two child nodes are at# index (2 * i + 1) and (2 * i + 2); the parent node is (i - 1) // 2# (where // indicates integer division).## The data arrays used here are as follows:#   data : the [n_samples x n_features] array of data from which the tree#          is built#   idx_array : the length n_samples array used to keep track of the indices#          of data within each node.  Each node has values idx_start and#          idx_end: the points within the node are given by (using numpy#          syntax) data[idx_array[idx_start:idx_end]].#   node_data : the length n_nodes array of structures which store the node#          indices, node radii, and leaf information for each node.#   node_bounds : the [* x n_nodes x n_features] array containing the node#          bound information.  For ball tree, the first dimension is 1, and#          each row contains the centroid of the node.  For kd tree, the first#          dimension is 2 and the rows for each point contain the arrays of#          lower bounds and upper bounds in each direction.## The lack of dynamic allocation means the number of nodes must be computed# before the building of the tree. This can be done assuming the points are# divided equally between child nodes at each step; although this removes# some flexibility in tree creation, it ensures a balanced tree and ensures# that the number of nodes required can be computed beforehand.  Given a# specified leaf_size (the minimum number of points in any node), it is# possible to show that a balanced tree will have##     n_levels = 1 + max(0, floor(log2((n_samples - 1) / leaf_size)))## in order to satisfy##     leaf_size <= min(n_points) <= 2 * leaf_size## with the exception of the special case where n_samples < leaf_size.# for a given number of levels, the number of nodes in the tree is given by##     n_nodes = 2 ** n_levels - 1## both these results can be straightforwardly shown by induction.  The# following code uses these values in the construction of the tree.## Distance Metrics# ----------------# For flexibility, the trees can be built using a variety of distance metrics.# The metrics are described in the DistanceMetric class: the standard# Euclidean distance is the default, and is inlined to be faster than other# metrics.  In addition, each metric defines both a distance and a# "reduced distance", which is often faster to compute, and is therefore# used in the query architecture whenever possible. (For example, in the# case of the standard Euclidean distance, the reduced distance is the# squared-distance).## Implementation Notes# --------------------# This implementation uses the common object-oriented approach of having an# abstract base class which is extended by the KDTree and BallTree# specializations.## The BinaryTree "base class" is defined here and then subclassed in the BallTree# and KDTree pyx files. These files include implementations of the# "abstract" methods.# Necessary Helper Functions# --------------------------# These are the names and descriptions of the "abstract" functions which are# defined in kd_tree.pyx and ball_tree.pyx:# cdef int allocate_data(BinaryTree tree, ITYPE_t n_nodes, ITYPE_t n_features):#     """Allocate arrays needed for the KD Tree"""# cdef int init_node(BinaryTree tree, ITYPE_t i_node,#                    ITYPE_t idx_start, ITYPE_t idx_end):#    """Initialize the node for the dataset stored in tree.data"""# cdef DTYPE_t min_rdist(BinaryTree tree, ITYPE_t i_node, DTYPE_t* pt):#     """Compute the minimum reduced-distance between a point and a node"""# cdef DTYPE_t min_dist(BinaryTree tree, ITYPE_t i_node, DTYPE_t* pt):#     """Compute the minimum distance between a point and a node"""# cdef DTYPE_t max_rdist(BinaryTree tree, ITYPE_t i_node, DTYPE_t* pt):#     """Compute the maximum reduced-distance between a point and a node"""# cdef DTYPE_t max_dist(BinaryTree tree, ITYPE_t i_node, DTYPE_t* pt):#     """Compute the maximum distance between a point and a node"""# cdef inline int min_max_dist(BinaryTree tree, ITYPE_t i_node, DTYPE_t* pt,#                              DTYPE_t* min_dist, DTYPE_t* max_dist):#     """Compute the minimum and maximum distance between a point and a node"""# cdef inline DTYPE_t min_rdist_dual(BinaryTree tree1, ITYPE_t i_node1,#                                    BinaryTree tree2, ITYPE_t i_node2):#     """Compute the minimum reduced distance between two nodes"""# cdef inline DTYPE_t min_dist_dual(BinaryTree tree1, ITYPE_t i_node1,#                                   BinaryTree tree2, ITYPE_t i_node2):#     """Compute the minimum distance between two nodes"""# cdef inline DTYPE_t max_rdist_dual(BinaryTree tree1, ITYPE_t i_node1,#                                    BinaryTree tree2, ITYPE_t i_node2):#     """Compute the maximum reduced distance between two nodes"""# cdef inline DTYPE_t max_dist_dual(BinaryTree tree1, ITYPE_t i_node1,#                                   BinaryTree tree2, ITYPE_t i_node2):#     """Compute the maximum distance between two nodes"""cimport numpy as cnpfrom libc.math cimport fabs, sqrt, exp, cos, pow, log, lgammafrom libc.math cimport fmin, fmaxfrom libc.stdlib cimport calloc, malloc, freefrom libc.string cimport memcpyimport numpy as npimport warningsctypedef cnp.float64_t DTYPE_tctypedef cnp.intp_t ITYPE_tctypedef cnp.int32_t SPARSE_INDEX_TYPE_tITYPE = np.intpDTYPE = np.float64from AKNN._partition_nodes cimport partition_node_indices####################################################################### Metric cdef inline DTYPE_t euclidean_dist(    const DTYPE_t* x1,    const DTYPE_t* x2,    ITYPE_t size,) nogil except -1:    cdef DTYPE_t tmp, d=0    cdef cnp.intp_t j    for j in range(size):        tmp = <DTYPE_t> (x1[j] - x2[j])        d += tmp * tmp    return sqrt(d)cdef inline DTYPE_t euclidean_rdist(    const DTYPE_t* x1,    const DTYPE_t* x2,    ITYPE_t size,) nogil except -1:    cdef DTYPE_t tmp, d=0    cdef cnp.intp_t j    for j in range(size):        tmp = <DTYPE_t>(x1[j] - x2[j])        d += tmp * tmp    return d############################################################################################################################################from sklearn.utils import check_array######################################################################from cython cimport floatingcdef inline void dual_swap(    floating* darr,    ITYPE_t *iarr,    ITYPE_t a,    ITYPE_t b,) nogil:    """Swap the values at index a and b of both darr and iarr"""    cdef floating dtmp = darr[a]    darr[a] = darr[b]    darr[b] = dtmp    cdef ITYPE_t itmp = iarr[a]    iarr[a] = iarr[b]    iarr[b] = itmpcdef int simultaneous_sort(    floating* values,    ITYPE_t* indices,    ITYPE_t size,) nogil:    """    Perform a recursive quicksort on the values array as to sort them ascendingly.    This simultaneously performs the swaps on both the values and the indices arrays.    The numpy equivalent is:        def simultaneous_sort(dist, idx):             i = np.argsort(dist)             return dist[i], idx[i]    Notes    -----    Arrays are manipulated via a pointer to there first element and their size    as to ease the processing of dynamically allocated buffers.    """    # TODO: In order to support discrete distance metrics, we need to have a    # simultaneous sort which breaks ties on indices when distances are identical.    # The best might be using a std::stable_sort and a Comparator which might need    # an Array of Structures (AoS) instead of the Structure of Arrays (SoA)    # currently used.    cdef:        ITYPE_t pivot_idx, i, store_idx        floating pivot_val    # in the small-array case, do things efficiently    if size <= 1:        pass    elif size == 2:        if values[0] > values[1]:            dual_swap(values, indices, 0, 1)    elif size == 3:        if values[0] > values[1]:            dual_swap(values, indices, 0, 1)        if values[1] > values[2]:            dual_swap(values, indices, 1, 2)            if values[0] > values[1]:                dual_swap(values, indices, 0, 1)    else:        # Determine the pivot using the median-of-three rule.        # The smallest of the three is moved to the beginning of the array,        # the middle (the pivot value) is moved to the end, and the largest        # is moved to the pivot index.        pivot_idx = size // 2        if values[0] > values[size - 1]:            dual_swap(values, indices, 0, size - 1)        if values[size - 1] > values[pivot_idx]:            dual_swap(values, indices, size - 1, pivot_idx)            if values[0] > values[size - 1]:                dual_swap(values, indices, 0, size - 1)        pivot_val = values[size - 1]        # Partition indices about pivot.  At the end of this operation,        # pivot_idx will contain the pivot value, everything to the left        # will be smaller, and everything to the right will be larger.        store_idx = 0        for i in range(size - 1):            if values[i] < pivot_val:                dual_swap(values, indices, i, store_idx)                store_idx += 1        dual_swap(values, indices, store_idx, size - 1)        pivot_idx = store_idx        # Recursively sort each side of the pivot        if pivot_idx > 1:            simultaneous_sort(values, indices, pivot_idx)        if pivot_idx + 2 < size:            simultaneous_sort(values + pivot_idx + 1,                              indices + pivot_idx + 1,                              size - pivot_idx - 1)    return 0######################################################################cdef extern from "numpy/arrayobject.h":    void PyArray_ENABLEFLAGS(cnp.ndarray arr, int flags)cnp.import_array()# some handy constantscdef DTYPE_t INF = np.infcdef DTYPE_t NEG_INF = -np.infcdef DTYPE_t PI = np.picdef DTYPE_t ROOT_2PI = sqrt(2 * PI)cdef DTYPE_t LOG_PI = log(PI)cdef DTYPE_t LOG_2PI = log(2 * PI)# Some compound datatypes used below:cdef struct NodeHeapData_t:    DTYPE_t val    ITYPE_t i1    ITYPE_t i2# build the corresponding numpy dtype for NodeHeapDatacdef NodeHeapData_t nhd_tmpNodeHeapData = np.asarray(<NodeHeapData_t[:1]>(&nhd_tmp)).dtypecdef struct NodeData_t:    ITYPE_t idx_start    ITYPE_t idx_end    ITYPE_t is_leaf    DTYPE_t radius# build the corresponding numpy dtype for NodeDatacdef NodeData_t nd_tmpNodeData = np.asarray(<NodeData_t[:1]>(&nd_tmp)).dtype####################################################################### heap push functioncdef inline int heap_push(    floating* values,    ITYPE_t* indices,    ITYPE_t size,    floating val,    ITYPE_t val_idx,) nogil:    """Push a tuple (val, val_idx) onto a fixed-size max-heap.    The max-heap is represented as a Structure of Arrays where:     - values is the array containing the data to construct the heap with     - indices is the array containing the indices (meta-data) of each value    Notes    -----    Arrays are manipulated via a pointer to there first element and their size    as to ease the processing of dynamically allocated buffers.    For instance, in pseudo-code:        values = [1.2, 0.4, 0.1],        indices = [42, 1, 5],        heap_push(            values=values,            indices=indices,            size=3,            val=0.2,            val_idx=4,        )    will modify values and indices inplace, giving at the end of the call:        values  == [0.4, 0.2, 0.1]        indices == [1, 4, 5]    """    cdef:        ITYPE_t current_idx, left_child_idx, right_child_idx, swap_idx    # Check if val should be in heap    if val >= values[0]:        return 0    # Insert val at position zero    values[0] = val    indices[0] = val_idx    # Descend the heap, swapping values until the max heap criterion is met    current_idx = 0    while True:        left_child_idx = 2 * current_idx + 1        right_child_idx = left_child_idx + 1        if left_child_idx >= size:            break        elif right_child_idx >= size:            if values[left_child_idx] > val:                swap_idx = left_child_idx            else:                break        elif values[left_child_idx] >= values[right_child_idx]:            if val < values[left_child_idx]:                swap_idx = left_child_idx            else:                break        else:            if val < values[right_child_idx]:                swap_idx = right_child_idx            else:                break        values[current_idx] = values[swap_idx]        indices[current_idx] = indices[swap_idx]        current_idx = swap_idx    values[current_idx] = val    indices[current_idx] = val_idx    return 0cdef inline int check_exceed(floating* a_dist_arr, ITYPE_t row,                        ITYPE_t* a_idx_arr, floating C,                       floating val, floating* largest_dist, floating beta                       )nogil:    cdef DTYPE_t k_float    if largest_dist[row]==INF:        if val>=a_dist_arr[row]:            k_float= a_idx_arr[row]            if val**beta * (k_float+1) <C:                a_dist_arr[row]=val                a_idx_arr[row]+=1            else:                largest_dist[row]=val        else:            a_idx_arr[row]+=1            k_float = a_idx_arr[row]            if a_dist_arr[row]**beta*(k_float+1) >=C:                largest_dist[row]=val    return 0####################################################################### Define doc strings, substituting the appropriate class name using# the DOC_DICT variable defined in the pyx files.CLASS_DOC = \"""{BinaryTree}(X, leaf_size=40, metric='minkowski', **kwargs){BinaryTree} for fast generalized N-point problemsRead more in the :ref:`User Guide <unsupervised_neighbors>`.Parameters----------X : array-like of shape (n_samples, n_features)    n_samples is the number of points in the data set, and    n_features is the dimension of the parameter space.    Note: if X is a C-contiguous array of doubles then data will    not be copied. Otherwise, an internal copy will be made.leaf_size : positive int, default=40    Number of points at which to switch to brute-force. Changing    leaf_size will not affect the results of a query, but can    significantly impact the speed of a query and the memory required    to store the constructed tree.  The amount of memory needed to    store the tree scales as approximately n_samples / leaf_size.    For a specified ``leaf_size``, a leaf node is guaranteed to    satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in    the case that ``n_samples < leaf_size``.metric : str or DistanceMetric object, default='minkowski'    Metric to use for distance computation. Default is "minkowski", which    results in the standard Euclidean distance when p = 2.    {binary_tree}.valid_metrics gives a list of the metrics which are valid for    {BinaryTree}. See the documentation of `scipy.spatial.distance    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and the    metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for    more information.Additional keywords are passed to the distance metric class.Note: Callable functions in the metric parameter are NOT supported for KDTreeand Ball Tree. Function call overhead will result in very poor performance.Attributes----------data : memory view    The training dataExamples--------Query for k-nearest neighbors    >>> import numpy as np    >>> from sklearn.neighbors import {BinaryTree}    >>> rng = np.random.RandomState(0)    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions    >>> tree = {BinaryTree}(X, leaf_size=2)              # doctest: +SKIP    >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP    >>> print(ind)  # indices of 3 closest neighbors    [0 3 1]    >>> print(dist)  # distances to 3 closest neighbors    [ 0.          0.19662693  0.29473397]Pickle and Unpickle a tree.  Note that the state of the tree is saved in thepickle operation: the tree needs not be rebuilt upon unpickling.    >>> import numpy as np    >>> import pickle    >>> rng = np.random.RandomState(0)    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions    >>> tree = {BinaryTree}(X, leaf_size=2)        # doctest: +SKIP    >>> s = pickle.dumps(tree)                     # doctest: +SKIP    >>> tree_copy = pickle.loads(s)                # doctest: +SKIP    >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP    >>> print(ind)  # indices of 3 closest neighbors    [0 3 1]    >>> print(dist)  # distances to 3 closest neighbors    [ 0.          0.19662693  0.29473397]Query for neighbors within a given radius    >>> import numpy as np    >>> rng = np.random.RandomState(0)    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions    >>> tree = {BinaryTree}(X, leaf_size=2)     # doctest: +SKIP    >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))    3    >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP    >>> print(ind)  # indices of neighbors within distance 0.3    [3 0 1]Compute a gaussian kernel density estimate:    >>> import numpy as np    >>> rng = np.random.RandomState(42)    >>> X = rng.random_sample((100, 3))    >>> tree = {BinaryTree}(X)                # doctest: +SKIP    >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')    array([ 6.94114649,  7.83281226,  7.2071716 ])Compute a two-point auto-correlation function    >>> import numpy as np    >>> rng = np.random.RandomState(0)    >>> X = rng.random_sample((30, 3))    >>> r = np.linspace(0, 1, 5)    >>> tree = {BinaryTree}(X)                # doctest: +SKIP    >>> tree.two_point_correlation(X, r)    array([ 30,  62, 278, 580, 820])"""cdef class NeighborsHeap:    """A max-heap structure to keep track of distances/indices of neighbors    This implements an efficient pre-allocated set of fixed-size heaps    for chasing neighbors, holding both an index and a distance.    When any row of the heap is full, adding an additional point will push    the furthest point off the heap.    Parameters    ----------    n_pts : int        the number of heaps to use    n_nbrs : int        the size of each heap.    """    cdef cnp.ndarray distances_arr    cdef cnp.ndarray indices_arr    cdef DTYPE_t[:, ::1] distances    cdef ITYPE_t[:, ::1] indices    def __cinit__(self):        self.distances_arr = np.zeros((1, 1), dtype=DTYPE, order='C')        self.indices_arr = np.zeros((1, 1), dtype=ITYPE, order='C')                self.distances = self.distances_arr        self.indices = self.indices_arr                 def __init__(self, n_pts, n_nbrs):        self.distances_arr = np.full((n_pts, n_nbrs), np.inf, dtype=DTYPE,                                     order='C')        self.indices_arr = np.zeros((n_pts, n_nbrs), dtype=ITYPE, order='C')        self.distances = self.distances_arr        self.indices = self.indices_arr                                 def get_arrays(self, sort=True):        """Get the arrays of distances and indices within the heap.        If sort=True, then simultaneously sort the indices and distances,        so the closer points are listed first.        """        if sort:            self._sort()        return self.distances_arr, self.indices_arr        cdef inline DTYPE_t largest(self, ITYPE_t row) nogil except -1:        """Return the largest distance in the given row"""        return self.distances[row, 0]    def push(self, ITYPE_t row, DTYPE_t val, ITYPE_t i_val):                return self._push(row, val, i_val)            cdef int _push(self, ITYPE_t row, DTYPE_t val,                   ITYPE_t i_val) nogil except -1:        """push (val, i_val) into the given row"""        cdef:            ITYPE_t size = self.distances.shape[1]            DTYPE_t* dist_arr = &self.distances[row, 0]            ITYPE_t* ind_arr = &self.indices[row, 0]        return heap_push(dist_arr, ind_arr, size, val, i_val)    cdef int _sort(self) except -1:        """simultaneously sort the distances and indices"""        cdef DTYPE_t[:, ::1] distances = self.distances        cdef ITYPE_t[:, ::1] indices = self.indices        cdef ITYPE_t row        for row in range(distances.shape[0]):            simultaneous_sort(&distances[row, 0],                               &indices[row, 0],                               distances.shape[1])        return 0cdef class AdaptiveNeighborsHeap:    """A max-heap structure to keep track of distances/indices of neighbors    This implements an efficient pre-allocated set of fixed-size heaps    for chasing neighbors, holding both an index and a distance.    When any row of the heap is full, adding an additional point will push    the furthest point off the heap.    Parameters    ----------    n_pts : int        the number of heaps to use    n_nbrs : int        the size of each heap.    """    cdef cnp.ndarray distances_arr    cdef cnp.ndarray indices_arr    cdef cnp.ndarray adaptive_max_idx_arr    cdef cnp.ndarray adaptive_max_distance_arr    cdef cnp.ndarray largest_dist_arr            cdef ITYPE_t[::1] adaptive_max_idx    cdef DTYPE_t[::1] adaptive_max_distance    cdef DTYPE_t[::1] largest_dist    cdef DTYPE_t[:, ::1] distances    cdef ITYPE_t[:, ::1] indices    cdef DTYPE_t C    cdef DTYPE_t beta    def __cinit__(self):        self.distances_arr = np.zeros((1, 1), dtype=DTYPE, order='C')        self.indices_arr = np.zeros((1, 1), dtype=ITYPE, order='C')                self.distances = self.distances_arr        self.indices = self.indices_arr                self.adaptive_max_idx_arr = np.zeros(2, dtype=ITYPE, order='C')        self.adaptive_max_distance_arr = np.zeros(2, dtype=DTYPE, order='C')                self.adaptive_max_idx = self.adaptive_max_idx_arr        self.adaptive_max_distance = self.adaptive_max_distance_arr                self.C = 1.5        self.beta = 1.5                self.largest_dist_arr = np.zeros(1, dtype=DTYPE, order='C')        self.largest_dist=self.largest_dist_arr                    # TODO: assert beta and C are positive    def __init__(self, n_pts, n_nbrs, C, beta):        self.distances_arr = np.full((n_pts, n_nbrs), np.inf, dtype=DTYPE,                                     order='C')        self.indices_arr = np.zeros((n_pts, n_nbrs), dtype=ITYPE, order='C')        self.distances = self.distances_arr        self.indices = self.indices_arr                self.adaptive_max_idx_arr = np.full(n_pts, 0,dtype=ITYPE)        self.adaptive_max_idx = self.adaptive_max_idx_arr                self.adaptive_max_distance_arr= np.full(n_pts, 0,dtype=DTYPE)        self.adaptive_max_distance=self.adaptive_max_distance_arr                         self.C = C        self.beta = beta                self.largest_dist_arr = np.full(n_pts,np.inf, dtype=DTYPE, order='C')        self.largest_dist = self.largest_dist_arr                            def get_arrays(self, sort=True):        """Get the arrays of distances and indices within the heap.        If sort=True, then simultaneously sort the indices and distances,        so the closer points are listed first.        """        if sort:            self._sort()        return self.distances_arr, self.indices_arr        '''    def get_finite_arrays(self,sort=True):        if sort:            self._sort()        non_inf_idx=(self.distances_arr<np.inf).sum()        return self.distances_arr[:non_inf_idx], self.indices_arr[:non_inf_idx]        '''    cdef inline DTYPE_t largest(self, ITYPE_t row) nogil except -1:        """Return the largest distance in the given row"""        return self.largest_dist[row]            def push(self, ITYPE_t row, DTYPE_t val, ITYPE_t i_val):        return self._push(row, val, i_val)    cdef int _push(self, ITYPE_t row, DTYPE_t val,                   ITYPE_t i_val) nogil except -1:        """push (val, i_val) into the given row"""                       cdef DTYPE_t* a_dist_arr =&self.adaptive_max_distance[0]        cdef ITYPE_t* a_idx_arr = &self.adaptive_max_idx[0]        cdef DTYPE_t threshold_const = self.C        cdef DTYPE_t* largest_dist= &self.largest_dist[0]                cdef DTYPE_t beta= self.beta                                                   check_exceed(a_dist_arr, row, a_idx_arr, threshold_const, val, largest_dist, beta)                                        cdef:            ITYPE_t size = self.distances.shape[1]            DTYPE_t* dist_arr = &self.distances[row, 0]            ITYPE_t* ind_arr = &self.indices[row, 0]        return heap_push(dist_arr, ind_arr, size, val, i_val)    cdef int _sort(self) except -1:        """simultaneously sort the distances and indices"""        cdef DTYPE_t[:, ::1] distances = self.distances        cdef ITYPE_t[:, ::1] indices = self.indices        cdef ITYPE_t row        for row in range(distances.shape[0]):            simultaneous_sort(&distances[row, 0],                               &indices[row, 0],                               distances.shape[1])        return 0#------------------------------------------------------------# find_node_split_dim:#  this computes the equivalent of#  j_max = np.argmax(np.max(data, 0) - np.min(data, 0))cdef ITYPE_t find_node_split_dim(DTYPE_t* data,                                 ITYPE_t* node_indices,                                 ITYPE_t n_features,                                 ITYPE_t n_points) except -1:    """Find the dimension with the largest spread.    Parameters    ----------    data : double pointer        Pointer to a 2D array of the training data, of shape [N, n_features].        N must be greater than any of the values in node_indices.    node_indices : int pointer        Pointer to a 1D array of length n_points.  This lists the indices of        each of the points within the current node.    Returns    -------    i_max : int        The index of the feature (dimension) within the node that has the        largest spread.    Notes    -----    In numpy, this operation is equivalent to    def find_node_split_dim(data, node_indices):        return np.argmax(data[node_indices].max(0) - data[node_indices].min(0))    The cython version is much more efficient in both computation and memory.    """    cdef DTYPE_t min_val, max_val, val, spread, max_spread    cdef ITYPE_t i, j, j_max    j_max = 0    max_spread = 0    for j in range(n_features):        max_val = data[node_indices[0] * n_features + j]        min_val = max_val        for i in range(1, n_points):            val = data[node_indices[i] * n_features + j]            max_val = fmax(max_val, val)            min_val = fmin(min_val, val)        spread = max_val - min_val        if spread > max_spread:            max_spread = spread            j_max = j    return j_max####################################################################### NodeHeap : min-heap used to keep track of nodes during#            breadth-first querycdef inline void swap_nodes(NodeHeapData_t* arr, ITYPE_t i1, ITYPE_t i2):    cdef NodeHeapData_t tmp = arr[i1]    arr[i1] = arr[i2]    arr[i2] = tmpcdef class NodeHeap:    """NodeHeap    This is a min-heap implementation for keeping track of nodes    during a breadth-first search.  Unlike the NeighborsHeap above,    the NodeHeap does not have a fixed size and must be able to grow    as elements are added.    Internally, the data is stored in a simple binary heap which meets    the min heap condition:        heap[i].val < min(heap[2 * i + 1].val, heap[2 * i + 2].val)    """    cdef cnp.ndarray data_arr    cdef NodeHeapData_t[::1] data    cdef ITYPE_t n    def __cinit__(self):        self.data_arr = np.zeros(1, dtype=NodeHeapData, order='C')        self.data = self.data_arr    def __init__(self, size_guess=100):        size_guess = max(size_guess, 1)  # need space for at least one item        self.data_arr = np.zeros(size_guess, dtype=NodeHeapData, order='C')        self.data = self.data_arr        self.n = size_guess        self.clear()    cdef int resize(self, ITYPE_t new_size) except -1:        """Resize the heap to be either larger or smaller"""        cdef NodeHeapData_t *data_ptr        cdef NodeHeapData_t *new_data_ptr        cdef ITYPE_t i        cdef ITYPE_t size = self.data.shape[0]        cdef cnp.ndarray new_data_arr = np.zeros(new_size,                                                dtype=NodeHeapData)        cdef NodeHeapData_t[::1] new_data = new_data_arr        if size > 0 and new_size > 0:            data_ptr = &self.data[0]            new_data_ptr = &new_data[0]            for i in range(min(size, new_size)):                new_data_ptr[i] = data_ptr[i]        if new_size < size:            self.n = new_size        self.data = new_data        self.data_arr = new_data_arr        return 0    cdef int push(self, NodeHeapData_t data) except -1:        """Push a new item onto the heap"""        cdef ITYPE_t i, i_parent        cdef NodeHeapData_t* data_arr        self.n += 1        if self.n > self.data.shape[0]:            self.resize(2 * self.n)        # put the new element at the end,        # and then perform swaps until the heap is in order        data_arr = &self.data[0]        i = self.n - 1        data_arr[i] = data        while i > 0:            i_parent = (i - 1) // 2            if data_arr[i_parent].val <= data_arr[i].val:                break            else:                swap_nodes(data_arr, i, i_parent)                i = i_parent        return 0    cdef NodeHeapData_t peek(self):        """Peek at the root of the heap, without removing it"""        return self.data[0]    cdef NodeHeapData_t pop(self):        """Remove the root of the heap, and update the remaining nodes"""        if self.n == 0:            raise ValueError('cannot pop on empty heap')        cdef ITYPE_t i, i_child1, i_child2, i_swap        cdef NodeHeapData_t* data_arr = &self.data[0]        cdef NodeHeapData_t popped_element = data_arr[0]        # pop off the first element, move the last element to the front,        # and then perform swaps until the heap is back in order        data_arr[0] = data_arr[self.n - 1]        self.n -= 1        i = 0        while (i < self.n):            i_child1 = 2 * i + 1            i_child2 = 2 * i + 2            i_swap = 0            if i_child2 < self.n:                if data_arr[i_child1].val <= data_arr[i_child2].val:                    i_swap = i_child1                else:                    i_swap = i_child2            elif i_child1 < self.n:                i_swap = i_child1            else:                break            if (i_swap > 0) and (data_arr[i_swap].val <= data_arr[i].val):                swap_nodes(data_arr, i, i_swap)                i = i_swap            else:                break        return popped_element    cdef void clear(self):        """Clear the heap"""        self.n = 0####################################################################### newObj function#  this is a helper function for picklingdef newObj(obj):    return obj.__new__(obj)####################################################################### Binary Tree classcdef class BinaryTree:    cdef cnp.ndarray data_arr       cdef cnp.ndarray idx_array_arr    cdef cnp.ndarray node_data_arr    cdef cnp.ndarray node_bounds_arr    cdef readonly const DTYPE_t[:, ::1] data    cdef public DTYPE_t sum_weight    # Even if those memoryviews attributes are const-qualified,    # they get modified via their numpy counterpart.    # For instance, `node_data` gets modified via `node_data_arr`.    cdef public const ITYPE_t[::1] idx_array    cdef public const NodeData_t[::1] node_data    cdef public const DTYPE_t[:, :, ::1] node_bounds    cdef ITYPE_t leaf_size    cdef ITYPE_t n_levels    cdef ITYPE_t n_nodes        cdef int euclidean    # variables to keep track of building & querying stats    cdef int n_trims    cdef int n_leaves    cdef int n_splits    cdef int n_calls    # Use cinit to initialize all arrays to empty: this will prevent memory    # errors and seg-faults in rare cases where __init__ is not called    def __cinit__(self):        self.data_arr = np.empty((1, 1), dtype=DTYPE, order='C')               self.idx_array_arr = np.empty(1, dtype=ITYPE, order='C')        self.node_data_arr = np.empty(1, dtype=NodeData, order='C')        self.node_bounds_arr = np.empty((1, 1, 1), dtype=DTYPE)        self.data = self.data_arr                self.idx_array = self.idx_array_arr        self.node_data = self.node_data_arr        self.node_bounds = self.node_bounds_arr        self.leaf_size = 0        self.n_levels = 0        self.n_nodes = 0        self.euclidean = False        self.n_trims = 0        self.n_leaves = 0        self.n_splits = 0        self.n_calls = 0    def __init__(self, data,                 leaf_size=40, metric='euclidean', **kwargs):        # validate data        self.data_arr = check_array(data, dtype=DTYPE, order='C')        if self.data_arr.size == 0:            raise ValueError("X is an empty array")        n_samples = self.data_arr.shape[0]        n_features = self.data_arr.shape[1]        if leaf_size < 1:            raise ValueError("leaf_size must be greater than or equal to 1")        self.leaf_size = leaf_size                   # determine number of levels in the tree, and from this        # the number of nodes in the tree.  This results in leaf nodes        # with numbers of points between leaf_size and 2 * leaf_size        self.n_levels = int(            np.log2(fmax(1, (n_samples - 1) / self.leaf_size)) + 1)        self.n_nodes = (2 ** self.n_levels) - 1        # allocate arrays for storage        self.idx_array_arr = np.arange(n_samples, dtype=ITYPE)        self.node_data_arr = np.zeros(self.n_nodes, dtype=NodeData)            self._update_memviews()        # Allocate tree-specific data        allocate_data(self, self.n_nodes, n_features)        self._recursive_build(            node_data=self.node_data_arr,            i_node=0,            idx_start=0,            idx_end=n_samples        )    def _update_memviews(self):        self.data = self.data_arr        self.idx_array = self.idx_array_arr        self.node_data = self.node_data_arr        self.node_bounds = self.node_bounds_arr    def get_arrays(self):        """        get_arrays()        Get data and node arrays.        Returns        -------        arrays: tuple of array            Arrays for storing tree data, index, node data and node bounds.        """        return (self.data_arr, self.idx_array_arr,                self.node_data_arr, self.node_bounds_arr)    cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,                             ITYPE_t size) nogil except -1:        """Compute the distance between arrays x1 and x2"""        self.n_calls += 1        return euclidean_dist(x1, x2, size)           cdef inline DTYPE_t rdist(self, DTYPE_t* x1, DTYPE_t* x2,                              ITYPE_t size) nogil except -1:        """Compute the reduced distance between arrays x1 and x2.        The reduced distance, defined for some metrics, is a quantity which        is more efficient to compute than the distance, but preserves the        relative rankings of the true distance.  For example, the reduced        distance for the Euclidean metric is the squared-euclidean distance.        """        self.n_calls += 1        return euclidean_rdist(x1, x2, size)          cdef int _recursive_build(self, NodeData_t[::1] node_data, ITYPE_t i_node, ITYPE_t idx_start,                              ITYPE_t idx_end) except -1:        """Recursively build the tree.        Parameters        ----------        i_node : int            the node for the current step        idx_start, idx_end : int            the bounding indices in the idx_array which define the points that            belong to this node.        """        cdef ITYPE_t imax        cdef ITYPE_t n_features = self.data.shape[1]        cdef ITYPE_t n_points = idx_end - idx_start        cdef ITYPE_t n_mid = n_points / 2        cdef ITYPE_t* idx_array = &self.idx_array[idx_start]        cdef DTYPE_t* data = &self.data[0, 0]        # initialize node data        init_node(self, node_data, i_node, idx_start, idx_end)        if 2 * i_node + 1 >= self.n_nodes:            node_data[i_node].is_leaf = True            if idx_end - idx_start > 2 * self.leaf_size:                # this shouldn't happen if our memory allocation is correct                # we'll proactively prevent memory errors, but raise a                # warning saying we're doing so.                import warnings                warnings.warn("Internal: memory layout is flawed: "                              "not enough nodes allocated")        elif idx_end - idx_start < 2:            # again, this shouldn't happen if our memory allocation            # is correct.  Raise a warning.            import warnings            warnings.warn("Internal: memory layout is flawed: "                          "too many nodes allocated")            node_data[i_node].is_leaf = True        else:            # split node and recursively construct child nodes.            node_data[i_node].is_leaf = False            i_max = find_node_split_dim(data, idx_array,                                        n_features, n_points)            partition_node_indices(data, idx_array, i_max, n_mid,                                   n_features, n_points)            self._recursive_build(node_data,2 * i_node + 1,                                  idx_start, idx_start + n_mid)            self._recursive_build(node_data, 2 * i_node + 2,                                  idx_start + n_mid, idx_end)    def query(self, X, k=1, return_distance=True,              dualtree=False, breadth_first=False,              sort_results=True):        """        query(X, k=1, return_distance=True,              dualtree=False, breadth_first=False)        query the tree for the k nearest neighbors        Parameters        ----------        X : array-like of shape (n_samples, n_features)            An array of points to query        k : int, default=1            The number of nearest neighbors to return        return_distance : bool, default=True            if True, return a tuple (d, i) of distances and indices            if False, return array i        dualtree : bool, default=False            if True, use the dual tree formalism for the query: a tree is            built for the query points, and the pair of trees is used to            efficiently search this space.  This can lead to better            performance as the number of points grows large.        breadth_first : bool, default=False            if True, then query the nodes in a breadth-first manner.            Otherwise, query the nodes in a depth-first manner.        sort_results : bool, default=True            if True, then distances and indices of each point are sorted            on return, so that the first column contains the closest points.            Otherwise, neighbors are returned in an arbitrary order.        Returns        -------        i    : if return_distance == False        (d,i) : if return_distance == True        d : ndarray of shape X.shape[:-1] + (k,), dtype=double            Each entry gives the list of distances to the neighbors of the            corresponding point.        i : ndarray of shape X.shape[:-1] + (k,), dtype=int            Each entry gives the list of indices of neighbors of the            corresponding point.        """        # XXX: we should allow X to be a pre-built tree.        X = check_array(X, dtype=DTYPE, order='C')        if X.shape[X.ndim - 1] != self.data.shape[1]:            raise ValueError("query data dimension must "                             "match training data dimension")        if self.data.shape[0] < k:            raise ValueError("k must be less than or equal "                             "to the number of training points")        # flatten X, and save original shape information        np_Xarr = X.reshape((-1, self.data.shape[1]))        cdef const DTYPE_t[:, ::1] Xarr = np_Xarr        cdef DTYPE_t reduced_dist_LB        cdef ITYPE_t i        cdef DTYPE_t* pt        # initialize heap for neighbors        cdef NeighborsHeap heap = NeighborsHeap(Xarr.shape[0], k)        # node heap for breadth-first queries        cdef NodeHeap nodeheap        if breadth_first:            nodeheap = NodeHeap(self.data.shape[0] // self.leaf_size)        # bounds is needed for the dual tree algorithm        cdef DTYPE_t[::1] bounds        self.n_trims = 0        self.n_leaves = 0        self.n_splits = 0               pt = &Xarr[0, 0]        if breadth_first:            for i in range(Xarr.shape[0]):                self._query_single_breadthfirst(pt, i, heap, nodeheap)                pt += Xarr.shape[1]        else:            with nogil:                for i in range(Xarr.shape[0]):                    reduced_dist_LB = min_rdist(self, 0, pt)                    self._query_single_depthfirst(0, pt, i, heap,                                                  reduced_dist_LB)                    pt += Xarr.shape[1]        distances, indices = heap.get_arrays(sort=sort_results)        #distances = self.dist_metric.rdist_to_dist(distances)        distances = distances**0.5                # deflatten results        if return_distance:            return (distances.reshape(X.shape[:X.ndim - 1] + (k,)),                    indices.reshape(X.shape[:X.ndim - 1] + (k,)))        else:            return indices.reshape(X.shape[:X.ndim - 1] + (k,))       def adaptive_query(self, X, C=1, beta=1,  max_neighbor=100, return_distance=True,              sort_results=True):        """        query(X, k=1, return_distance=True,              dualtree=False, breadth_first=False)        query the tree for the k nearest neighbors        Parameters        ----------        X : array-like of shape (n_samples, n_features)            An array of points to query        k : int, default=1            The number of nearest neighbors to return        return_distance : bool, default=True            if True, return a tuple (d, i) of distances and indices            if False, return array i        dualtree : bool, default=False            if True, use the dual tree formalism for the query: a tree is            built for the query points, and the pair of trees is used to            efficiently search this space.  This can lead to better            performance as the number of points grows large.        breadth_first : bool, default=False            if True, then query the nodes in a breadth-first manner.            Otherwise, query the nodes in a depth-first manner.        sort_results : bool, default=True            if True, then distances and indices of each point are sorted            on return, so that the first column contains the closest points.            Otherwise, neighbors are returned in an arbitrary order.        Returns        -------        i    : if return_distance == False        (d,i) : if return_distance == True        d : ndarray of shape X.shape[:-1] + (k,), dtype=double            Each entry gives the list of distances to the neighbors of the            corresponding point.        i : ndarray of shape X.shape[:-1] + (k,), dtype=int            Each entry gives the list of indices of neighbors of the            corresponding point.        """        # XXX: we should allow X to be a pre-built tree.        X = check_array(X, dtype=DTYPE, order='C')                return_shape=min(self.data.shape[0],max_neighbor)        if X.shape[X.ndim - 1] != self.data.shape[1]:            raise ValueError("query data dimension must "                             "match training data dimension")          # flatten X, and save original shape information        np_Xarr = X.reshape((-1, self.data.shape[1]))        cdef const DTYPE_t[:, ::1] Xarr = np_Xarr        cdef DTYPE_t reduced_dist_LB        cdef ITYPE_t i        cdef DTYPE_t* pt                        cdef DTYPE_t C_ctype = C        cdef DTYPE_t beta_ctype = beta              # initialize heap for neighbors        cdef AdaptiveNeighborsHeap heap = AdaptiveNeighborsHeap(Xarr.shape[0], return_shape, C_ctype, beta_ctype)                     self.n_trims = 0        self.n_leaves = 0        self.n_splits = 0               pt = &Xarr[0, 0]              cdef ITYPE_t check_value        with nogil:            for i in range(Xarr.shape[0]):                reduced_dist_LB = min_rdist(self, 0, pt)                self._adaptive_query_single_depthfirst(0, pt, i, heap,                                              reduced_dist_LB)                                pt += Xarr.shape[1]                                     distances, indices = heap.get_arrays(sort=sort_results)        #distances = self.dist_metric.rdist_to_dist(distances)        distances = distances**0.5            adaptive_distance=[]        for i in range(distances.shape[0]):            statistic_vec=[distances[i,j]**beta*(j+1) for j in range(len(distances[i]))]            non_inf_idx=np.array(statistic_vec)<C            if non_inf_idx.sum()==0:                raise ValueError("Given condition is too restrctive, no number of neighbors feasible")            else:                adaptive_distance.append(distances[i][non_inf_idx].max())                return np.array(adaptive_distance)        #if return_distance:        #    return (distances.reshape(X.shape[:X.ndim - 1] + (return_shape,)),        #            indices.reshape(X.shape[:X.ndim - 1] + (return_shape,)))        #else:        #    return indices.reshape(X.shape[:X.ndim - 1] + (return_shape,))        cdef int _query_single_depthfirst(self, ITYPE_t i_node,                                      DTYPE_t* pt, ITYPE_t i_pt,                                      NeighborsHeap heap,                                      DTYPE_t reduced_dist_LB) nogil except -1:        """Recursive Single-tree k-neighbors query, depth-first approach"""        cdef NodeData_t node_info = self.node_data[i_node]        cdef DTYPE_t dist_pt, reduced_dist_LB_1, reduced_dist_LB_2        cdef ITYPE_t i, i1, i2        cdef DTYPE_t* data = &self.data[0, 0]        #------------------------------------------------------------        # Case 1: query point is outside node radius:        #         trim it from the query        if reduced_dist_LB > heap.largest(i_pt):            self.n_trims += 1        #------------------------------------------------------------        # Case 2: this is a leaf node.  Update set of nearby points        elif node_info.is_leaf:            self.n_leaves += 1            for i in range(node_info.idx_start, node_info.idx_end):                dist_pt = self.rdist(pt,                                     &self.data[self.idx_array[i], 0],                                     self.data.shape[1])                heap._push(i_pt, dist_pt, self.idx_array[i])        #------------------------------------------------------------        # Case 3: Node is not a leaf.  Recursively query subnodes        #         starting with the closest        else:            self.n_splits += 1            i1 = 2 * i_node + 1            i2 = i1 + 1            reduced_dist_LB_1 = min_rdist(self, i1, pt)            reduced_dist_LB_2 = min_rdist(self, i2, pt)            # recursively query subnodes            if reduced_dist_LB_1 <= reduced_dist_LB_2:                self._query_single_depthfirst(i1, pt, i_pt, heap,                                              reduced_dist_LB_1)                self._query_single_depthfirst(i2, pt, i_pt, heap,                                              reduced_dist_LB_2)            else:                self._query_single_depthfirst(i2, pt, i_pt, heap,                                              reduced_dist_LB_2)                self._query_single_depthfirst(i1, pt, i_pt, heap,                                              reduced_dist_LB_1)        return 0    cdef int _query_single_breadthfirst(self, DTYPE_t* pt,                                        ITYPE_t i_pt,                                        NeighborsHeap heap,                                        NodeHeap nodeheap) except -1:        """Non-recursive single-tree k-neighbors query, breadth-first search"""        cdef ITYPE_t i, i_node        cdef DTYPE_t dist_pt, reduced_dist_LB        cdef NodeData_t* node_data = &self.node_data[0]        cdef DTYPE_t* data = &self.data[0, 0]        # Set up the node heap and push the head node onto it        cdef NodeHeapData_t nodeheap_item        nodeheap_item.val = min_rdist(self, 0, pt)        nodeheap_item.i1 = 0        nodeheap.push(nodeheap_item)        while nodeheap.n > 0:            nodeheap_item = nodeheap.pop()            reduced_dist_LB = nodeheap_item.val            i_node = nodeheap_item.i1            node_info = node_data[i_node]            #------------------------------------------------------------            # Case 1: query point is outside node radius:            #         trim it from the query            if reduced_dist_LB > heap.largest(i_pt):                self.n_trims += 1            #------------------------------------------------------------            # Case 2: this is a leaf node.  Update set of nearby points            elif node_data[i_node].is_leaf:                self.n_leaves += 1                for i in range(node_data[i_node].idx_start,                               node_data[i_node].idx_end):                    dist_pt = self.rdist(pt,                                         &self.data[self.idx_array[i], 0],                                         self.data.shape[1])                    heap._push(i_pt, dist_pt, self.idx_array[i])            #------------------------------------------------------------            # Case 3: Node is not a leaf.  Add subnodes to the node heap            else:                self.n_splits += 1                for i in range(2 * i_node + 1, 2 * i_node + 3):                    nodeheap_item.i1 = i                    nodeheap_item.val = min_rdist(self, i, pt)                    nodeheap.push(nodeheap_item)        return 0    cdef int _adaptive_query_single_depthfirst(self, ITYPE_t i_node,                                      DTYPE_t* pt, ITYPE_t i_pt,                                      AdaptiveNeighborsHeap heap,                                      DTYPE_t reduced_dist_LB) nogil except -1:                        """Recursive Single-tree k-neighbors query, depth-first approach"""        cdef NodeData_t node_info = self.node_data[i_node]                cdef DTYPE_t dist_pt, reduced_dist_LB_1, reduced_dist_LB_2        cdef ITYPE_t i, i1, i2        cdef DTYPE_t* data = &self.data[0, 0]        #------------------------------------------------------------        # Case 1: query point is outside node radius:        #         trim it from the query        if reduced_dist_LB > heap.largest(i_pt):            self.n_trims += 1        #------------------------------------------------------------        # Case 2: this is a leaf node.  Update set of nearby points        elif node_info.is_leaf:            self.n_leaves += 1            for i in range(node_info.idx_start, node_info.idx_end):                dist_pt = self.rdist(pt,                                     &self.data[self.idx_array[i], 0],                                     self.data.shape[1])                                            heap._push(i_pt, dist_pt, self.idx_array[i])                       #------------------------------------------------------------        # Case 3: Node is not a leaf.  Recursively query subnodes        #         starting with the closest        else:            self.n_splits += 1            i1 = 2 * i_node + 1            i2 = i1 + 1            reduced_dist_LB_1 = min_rdist(self, i1, pt)            reduced_dist_LB_2 = min_rdist(self, i2, pt)            # recursively query subnodes            if reduced_dist_LB_1 <= reduced_dist_LB_2:                self._adaptive_query_single_depthfirst(i1, pt, i_pt, heap,                                              reduced_dist_LB_1)                self._adaptive_query_single_depthfirst(i2, pt, i_pt, heap,                                              reduced_dist_LB_2)            else:                self._adaptive_query_single_depthfirst(i2, pt, i_pt, heap,                                              reduced_dist_LB_2)                self._adaptive_query_single_depthfirst(i1, pt, i_pt, heap,                                              reduced_dist_LB_1)        return 0      